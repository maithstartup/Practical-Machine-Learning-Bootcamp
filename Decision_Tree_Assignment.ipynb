{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Decision Tree Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UgGSDVVoFMof",
        "vBSOlvtYFMor",
        "1XjXuGzSFMox",
        "2xP8fMzlFMo2",
        "U3syV_ceFMo6",
        "396yf2D6FMo7",
        "pZA01gq9FMo-",
        "L0WQgnrcFMpC",
        "Tc0MDcfgFMpM",
        "JCZU4xvaFMpY",
        "Eko7NtnjFMpg",
        "DX9MdnIgFMpy",
        "FEJTNbZHFMp1",
        "sciSE7vuFMp6"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maithstartup/Practical-Machine-Learning-Bootcamp/blob/master/Decision_Tree_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwgTnssCFMny",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "\n",
        "#  Introduction to Decision Trees\n",
        "\n",
        "\n",
        "\n",
        "Decision Trees are an important type of algorithm for predictive modeling machine learning.\n",
        "\n",
        "The classical decision tree algorithms have been around for decades and modern variations like random forest are among the most powerful techniques available.\n",
        "\n",
        "Classification and Regression Trees or `CART` for short is a term introduced by `Leo Breiman` to refer to Decision Tree algorithms that can be used for classification or regression predictive modeling problems.\n",
        "\n",
        "Classically, this algorithm is referred to as “`decision trees`”, but on some platforms like R they are referred to by the more modern term CART.\n",
        "\n",
        "The `CART` algorithm provides a foundation for important algorithms like `bagged decision trees`, `random forest` and `boosted decision trees`.\n",
        "\n",
        "### CART Model Representation\n",
        "The representation for the CART model is a binary tree.\n",
        "\n",
        "This is your binary tree from algorithms and data structures, nothing too fancy. Each root node represents a single input variable (x) and a split point on that variable (assuming the variable is numeric).\n",
        "\n",
        "The leaf nodes of the tree contain an output variable (y) which is used to make a prediction.\n",
        "\n",
        "Given a new input, the tree is traversed by evaluating the specific input started at the root node of the tree.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64BYJZyqFMn0",
        "colab_type": "text"
      },
      "source": [
        "# Decision Tree Implementation in python\n",
        "\n",
        "We will use Decision Tree in Predicting the attrition of your valuable employees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFBqV_ySFMn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.style.use(\"fivethirtyeight\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxjpE5laFMn5",
        "colab_type": "text"
      },
      "source": [
        "# Problem Statement\n",
        "\n",
        "The key to success in any organization is attracting and retaining top talent. Our task is to classify which employess are likely to leave the compay and determine which factors keep employees at my company and which prompt others to leave. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6VzmNlHOFMn6",
        "colab_type": "code",
        "outputId": "81bb20e1-1694-4885-8178-656455a62070",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"HR-Employee-Attrition.csv\")         #change the directory to the location of the file \n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Attrition</th>\n",
              "      <th>BusinessTravel</th>\n",
              "      <th>DailyRate</th>\n",
              "      <th>Department</th>\n",
              "      <th>DistanceFromHome</th>\n",
              "      <th>Education</th>\n",
              "      <th>EducationField</th>\n",
              "      <th>EmployeeCount</th>\n",
              "      <th>EmployeeNumber</th>\n",
              "      <th>...</th>\n",
              "      <th>RelationshipSatisfaction</th>\n",
              "      <th>StandardHours</th>\n",
              "      <th>StockOptionLevel</th>\n",
              "      <th>TotalWorkingYears</th>\n",
              "      <th>TrainingTimesLastYear</th>\n",
              "      <th>WorkLifeBalance</th>\n",
              "      <th>YearsAtCompany</th>\n",
              "      <th>YearsInCurrentRole</th>\n",
              "      <th>YearsSinceLastPromotion</th>\n",
              "      <th>YearsWithCurrManager</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>41</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Travel_Rarely</td>\n",
              "      <td>1102</td>\n",
              "      <td>Sales</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>Life Sciences</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>49</td>\n",
              "      <td>No</td>\n",
              "      <td>Travel_Frequently</td>\n",
              "      <td>279</td>\n",
              "      <td>Research &amp; Development</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>Life Sciences</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>80</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>37</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Travel_Rarely</td>\n",
              "      <td>1373</td>\n",
              "      <td>Research &amp; Development</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Other</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>33</td>\n",
              "      <td>No</td>\n",
              "      <td>Travel_Frequently</td>\n",
              "      <td>1392</td>\n",
              "      <td>Research &amp; Development</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>Life Sciences</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>27</td>\n",
              "      <td>No</td>\n",
              "      <td>Travel_Rarely</td>\n",
              "      <td>591</td>\n",
              "      <td>Research &amp; Development</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>Medical</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>80</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 35 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
              "0   41       Yes      Travel_Rarely       1102                   Sales   \n",
              "1   49        No  Travel_Frequently        279  Research & Development   \n",
              "2   37       Yes      Travel_Rarely       1373  Research & Development   \n",
              "3   33        No  Travel_Frequently       1392  Research & Development   \n",
              "4   27        No      Travel_Rarely        591  Research & Development   \n",
              "\n",
              "   DistanceFromHome  Education EducationField  EmployeeCount  EmployeeNumber  \\\n",
              "0                 1          2  Life Sciences              1               1   \n",
              "1                 8          1  Life Sciences              1               2   \n",
              "2                 2          2          Other              1               4   \n",
              "3                 3          4  Life Sciences              1               5   \n",
              "4                 2          1        Medical              1               7   \n",
              "\n",
              "   ...  RelationshipSatisfaction StandardHours  StockOptionLevel  \\\n",
              "0  ...                         1            80                 0   \n",
              "1  ...                         4            80                 1   \n",
              "2  ...                         2            80                 0   \n",
              "3  ...                         3            80                 0   \n",
              "4  ...                         4            80                 1   \n",
              "\n",
              "   TotalWorkingYears  TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n",
              "0                  8                      0               1               6   \n",
              "1                 10                      3               3              10   \n",
              "2                  7                      3               3               0   \n",
              "3                  8                      3               3               8   \n",
              "4                  6                      3               3               2   \n",
              "\n",
              "  YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n",
              "0                  4                        0                     5  \n",
              "1                  7                        1                     7  \n",
              "2                  0                        0                     0  \n",
              "3                  7                        3                     0  \n",
              "4                  2                        2                     2  \n",
              "\n",
              "[5 rows x 35 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y0C3ibmFMn_",
        "colab_type": "text"
      },
      "source": [
        "## 1. Exploratory Data Analysis\n",
        "\n",
        "The below table shows us that the dataset consists  of 1470 rows and 35 columns. It also gives us information whether the features are categorical or numerical and indicates there or no missing values in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "id": "6toPOOxJFMoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df.shape)\n",
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txGrYoozFMoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
        "df.describe() #gives the descriptive statistics of the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i15oHnvPFMoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop(['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'], axis=\"columns\", inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwK1Vq5QFMoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_col = []\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == object and len(df[column].unique()) <= 50:\n",
        "        categorical_col.append(column)\n",
        "        print(f\"{column} : {df[column].unique()}\")\n",
        "        print(\"====================================\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCwStd8xFMoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Attrition'] = df.Attrition.astype(\"category\").cat.codes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da1UpgziFMoS",
        "colab_type": "text"
      },
      "source": [
        "## 2. Data Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRny91cHFMoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.Attrition.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKRPsqdaFMoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visulazing the distibution of the data for every feature\n",
        "df.hist(edgecolor='black', linewidth=1.2, figsize=(20, 20));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci4nXqB9FMoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting how every feature correlate with the \"target\"\n",
        "sns.set(font_scale=1.2)\n",
        "plt.figure(figsize=(30, 30))\n",
        "\n",
        "for i, column in enumerate(categorical_col, 1):\n",
        "    plt.subplot(3, 3, i)\n",
        "    g = sns.barplot(x=f\"{column}\", y='Attrition', data=df)\n",
        "    g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
        "    plt.ylabel('Attrition Count')\n",
        "    plt.xlabel(f'{column}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IDjaL8dFMof",
        "colab_type": "text"
      },
      "source": [
        "**Conclusions:**\n",
        "\n",
        "***\n",
        "- `BusinessTravel` : The workers who travel alot are more likely to quit then other employees.\n",
        "\n",
        "- `Department` : The worker in `Research & Development` are more likely to stay then the workers on other departement.\n",
        "\n",
        "- `EducationField` : The workers with `Human Resources` and `Technical Degree` are more likely to quit then employees from other fields of educations.\n",
        "\n",
        "- `Gender` : The `Male` are more likely to quit.\n",
        "\n",
        "- `JobRole` : The workers in `Laboratory Technician`, `Sales Representative`, and `Human Resources` are more likely to quit the workers in other positions.\n",
        "\n",
        "- `MaritalStatus` : The workers who have `Single` marital status are more likely to quit the `Married`, and `Divorced`.\n",
        "\n",
        "- `OverTime` : The workers who work more hours are likely to quit then others.\n",
        "\n",
        "*** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgGSDVVoFMof",
        "colab_type": "text"
      },
      "source": [
        "## 3. Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_gS6yr9FMoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_col.remove('Attrition')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "id": "Gr18J9u5FMok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform categorical data into dummies\n",
        "# categorical_col.remove(\"Attrition\")\n",
        "# data = pd.get_dummies(df, columns=categorical_col)\n",
        "# data.info()\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label = LabelEncoder()\n",
        "for column in categorical_col:\n",
        "    df[column] = label.fit_transform(df[column])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNbjlsTTFMon",
        "colab_type": "text"
      },
      "source": [
        "The label encoder converts all categorical variables into numerical variables as each class corresponding to a number for training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE26SXoUFMoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.drop('Attrition', axis=1)\n",
        "y = df.Attrition"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBSOlvtYFMor",
        "colab_type": "text"
      },
      "source": [
        "## 4. Applying machine learning algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP2lfSS-FMor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n",
        "    if train:\n",
        "        pred = clf.predict(X_train)\n",
        "        print(\"Train Result:\\n===========================================\")\n",
        "        print(f\"accuracy score: {accuracy_score(y_train, pred):.4f}\\n\")\n",
        "        print(f\"Classification Report: \\n \\tPrecision: {precision_score(y_train, pred)}\\n\\tRecall Score: {recall_score(y_train, pred)}\\n\\tF1 score: {f1_score(y_train, pred)}\\n\")\n",
        "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, clf.predict(X_train))}\\n\")\n",
        "        \n",
        "    elif train==False:\n",
        "        pred = clf.predict(X_test)\n",
        "        print(\"Test Result:\\n===========================================\")        \n",
        "        print(f\"accuracy score: {accuracy_score(y_test, pred)}\\n\")\n",
        "        print(f\"Classification Report: \\n \\tPrecision: {precision_score(y_test, pred)}\\n\\tRecall Score: {recall_score(y_test, pred)}\\n\\tF1 score: {f1_score(y_test, pred)}\\n\")\n",
        "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyaP9x9YFMou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XjXuGzSFMox",
        "colab_type": "text"
      },
      "source": [
        "## 5 Decision Tree Classifier Parameters\n",
        "\n",
        "- `criterion`: The function to measure the quality of a split. Supported criteria are \"`gini`\" for the Gini impurity and \"`entropy`\" for the information gain.\n",
        "***\n",
        "- `splitter`: The strategy used to choose the split at each node. Supported strategies are \"`best`\" to choose the best split and \"`random`\" to choose the best random split.\n",
        "***\n",
        "- `max_depth`: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than `min_samples_split` samples.\n",
        "***\n",
        "- `min_samples_split`: The minimum number of samples required to split an internal node.\n",
        "***\n",
        "- `min_samples_leaf`: The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression.\n",
        "***\n",
        "- `min_weight_fraction_leaf`: The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
        "***\n",
        "- `max_features`: The number of features to consider when looking for the best split.\n",
        "***\n",
        "- `max_leaf_nodes`: Grow a tree with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
        "***\n",
        "- `min_impurity_decrease`: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
        "***\n",
        "- `min_impurity_split`: Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpS1xYf2FMoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "print_score(tree, X_train, y_train, X_test, y_test, train=True)\n",
        "print_score(tree, X_train, y_train, X_test, y_test, train=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImerxKrxFMo1",
        "colab_type": "text"
      },
      "source": [
        "Confusion matrix and F1 score is used as a evaluation metric as the dataset is imbalanced. From the above results, it can be inferred that when we train using the default parameters of the DecisionTreeClassifier without restricting parameters like max_depth and min_samples_leaf the model overfits. This overfitting can be understood from the train accuracy and F1 score being 1 which shows that no point has been misclassified in training data. The very low F1 score of 0.28 indicates that the model performs very badly on test data. Our aim is to achieve a trade-off between train and test accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xP8fMzlFMo2",
        "colab_type": "text"
      },
      "source": [
        "### Visualization of a tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odDR4qsVFMo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "features = list(df.columns)\n",
        "features.remove(\"Attrition\")\n",
        "classes=['Yes','No']\n",
        "dot_data = export_graphviz(tree, out_file=None, \n",
        "                     feature_names=features,\n",
        "                     class_names=classes,      \n",
        "                     filled=True, rounded=True,  \n",
        "                     special_characters=True)  \n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph.view()\n",
        "graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQV49zo0FMo5",
        "colab_type": "text"
      },
      "source": [
        "The decision tree can be visualized using the graphviz library. At each node (decision) of the tree, we get the variable of splitting with the value, gini index of the samples of the dataset at that node, no of samples in each class and the majority class. A decision on which class (Attrition: Yes or No) a data point belongs to can be found by traversing through one branch of the tree.The above tree which has been trained with default parameters has a depth of 16. Higher the depth, more the chances of overfitting. The overfitting can be seen in the leaf nodes where the no of samples in each class is a very small number like one or two. This implies the dataset has an high risk of overfitting to the noisy points in the data.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3syV_ceFMo6",
        "colab_type": "text"
      },
      "source": [
        "## 5.1 Decision Tree Classifier Hyper-Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "396yf2D6FMo7",
        "colab_type": "text"
      },
      "source": [
        "## Criterion\n",
        "\n",
        "The time taken for the decision tree classifier for training and the accuracy with gini impurity as the criterion is given in the below code. The task is to change the criterion to entropy and find the accuracy and training time. Compare the results and write your inferences on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5KzMZniFMo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "t0 = time.time()\n",
        "tree = DecisionTreeClassifier(random_state=42,max_depth = 3)\n",
        "tree.fit(X_train, y_train)\n",
        "print(\"Training time using gini criterion\", round(time.time()-t0, 5), \"s\")\n",
        "print('Accuracy using the defualt gini impurity criterion\\n')\n",
        "print_score(tree, X_train, y_train, X_test, y_test, train=False)\n",
        "\n",
        "##################write your code here######################\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrO8v6-FMo-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZA01gq9FMo-",
        "colab_type": "text"
      },
      "source": [
        "## Splitter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KQh5kpYyFMo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = time.time()\n",
        "clf = DecisionTreeClassifier(random_state= 42,max_depth = 3, criterion = \"entropy\", splitter = 'best')\n",
        "clf.fit(X_train,y_train)\n",
        "print('Best Split running time...',time.time() - t)\n",
        "print('Best Split accuracy...',clf.score(X_test,y_test))\n",
        "\n",
        "t = time.time()\n",
        "clf = DecisionTreeClassifier(random_state= 42,max_depth = 3, criterion = \"entropy\", splitter = 'random')\n",
        "clf.fit(X_train,y_train)\n",
        "print('Random Split running time...',time.time() - t)\n",
        "print('Random Split accuracy...',clf.score(X_test,y_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mRJdhVJFMpC",
        "colab_type": "text"
      },
      "source": [
        "Even though there is a drop in the accuracy while using random split, random split decreases the training time significantly compared to best split as it takes a random subset of features for computing the Information gain. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0WQgnrcFMpC",
        "colab_type": "text"
      },
      "source": [
        "## Max Depth\n",
        "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWAwyfEOFMpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_score = []\n",
        "train_score = []\n",
        "for depth in range(20):\n",
        "    clf = DecisionTreeClassifier(random_state= 42,max_depth = depth + 1)\n",
        "    clf.fit(X_train,y_train)\n",
        "    train_score.append(clf.score(X_train,y_train))\n",
        "    test_score.append(clf.score(X_test,y_test))\n",
        "\n",
        "plt.figure(figsize = (6,6))\n",
        "plt.plot(range(20),train_score)\n",
        "plt.plot(range(20), test_score)\n",
        "plt.xlabel('Tree Depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Training set','Test set'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVPmdsErFMpG",
        "colab_type": "text"
      },
      "source": [
        "At a depth of around 5, a good tradeoff between test and train accuracy is obtained. At a depth of 1 or 2, the tree is shallow and underfits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYAn1U8SFMpG",
        "colab_type": "text"
      },
      "source": [
        "The task is to build a decision tree with optimal depth as mentioned above and other suitable values for the parameters criterion and splitter. Visualize the classifier using export_graphviz() and write your inferences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzlblpMlFMpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################write your code here######################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#graph.view()      #Uncomment text to view it as pdfgraph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRbn6y4JFMpL",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc0MDcfgFMpM",
        "colab_type": "text"
      },
      "source": [
        "## Max_Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZAeS4u4FMpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_score = []\n",
        "train_score = []\n",
        "max_features = range(len(df.columns)-1)\n",
        "for feat in max_features:\n",
        "    clf = DecisionTreeClassifier(random_state= 42,max_features = feat + 1,max_depth=5,criterion = \"entropy\", splitter = 'best')\n",
        "    clf.fit(X_train,y_train)\n",
        "    train_score.append(clf.score(X_train,y_train))\n",
        "    test_score.append(clf.score(X_test,y_test))\n",
        "\n",
        "plt.figure(figsize = (6,6))\n",
        "plt.plot(max_features,train_score)\n",
        "plt.plot(max_features, test_score)\n",
        "plt.xlabel('Max Features')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Training set','Test set'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW60vN5_FMpQ",
        "colab_type": "text"
      },
      "source": [
        "The max_features set to 3 gives a good accuracy on both the train and test set. This shows that comparing Information gain for just 3 features during a split can fit a really good model and also save a lot of computing time. But conclusive evidence on how many features is to be used for splitting can't be obtained from the graph as it is noisy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNYYtuqkFMpQ",
        "colab_type": "text"
      },
      "source": [
        "# Min sample leaf\n",
        "The minimum number of samples required to be at a leaf node: If int, then consider min_samples_leaf as the minimum number. If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
        "\n",
        "The task is to find the optimal minimum number of samples required to be at a leaf node from the given min_sample_leaf values. Calculate the train and test scores of the model and plot the same"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noXvUFsQFMpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_score = []\n",
        "train_score = []\n",
        "min_sample_leaf = np.arange(5,100,5)\n",
        "##################write your code here######################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize = (8,8))\n",
        "plt.plot(min_sample_leaf,train_score)\n",
        "plt.plot(min_sample_leaf, test_score)\n",
        "plt.xlabel('Min Sample Leaf')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Training set','Test set'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgcOANUHFMpU",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33fnhz1PFMpU",
        "colab_type": "text"
      },
      "source": [
        "The task is to build a decision tree with optimal min_samples_leaf =20 as mentioned above and other suitable values for the parameters criterion, splitter and depth. Visualize the classifier using export_graphviz() and write your inferences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1_rffA7FMpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################write your code here######################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XC1iEjUFMpY",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCZU4xvaFMpY",
        "colab_type": "text"
      },
      "source": [
        "## Training Decision tree with optimal parameters\n",
        "\n",
        "Let us check the accuracy and F1 scores of the above model with best fit parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ348V7-FMpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tree = DecisionTreeClassifier(random_state=42,criterion = \"entropy\",max_depth = 5,min_samples_leaf =20, splitter = 'best')\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "print_score(tree, X_train, y_train, X_test, y_test, train=True)\n",
        "print_score(tree, X_train, y_train, X_test, y_test, train=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSf9fB1jFMpc",
        "colab_type": "text"
      },
      "source": [
        "As the dataset is imbalanced, F1 score should also be considered while evaluating the model. Even though the train and test accuracies are high, F1 score values are less than 0.5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR54gkp2FMpd",
        "colab_type": "text"
      },
      "source": [
        "To solve the imbalance in the dataset, set the class weight parameter to balanced in the above model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLt5XQkgFMpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################write your code here######################\n",
        "\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "print_score(tree, X_train, y_train, X_test, y_test, train=True)\n",
        "print_score(tree, X_train, y_train, X_test, y_test, train=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1ljyBV5FMpg",
        "colab_type": "text"
      },
      "source": [
        "F1 score can be increased by using the parameter class_weight = 'balanced' which penalizes mistakes in samples of class[i] with class_weight[i] instead of 1. This gives higher weights to the minority class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eko7NtnjFMpg",
        "colab_type": "text"
      },
      "source": [
        "## 5.2. Decision Tree Classifier Hyperparameter tuning using GridSearchCV\n",
        "GridSearchCV is a library function that is a member of sklearn’s model_selection package. It helps to loop through predefined hyperparameters and fit your estimator (model) on your training set. So, in the end, you can select the best parameters from the listed hyperparameters.In addition to that, you can specify the number of times for the cross-validation for each set of hyperparameters. The below model fits for all combinations of the 5 parameters given below with a 3-fold cross validation and the best estimators are obtained using Grid search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQV7p-4lFMph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {\n",
        "    \"criterion\":(\"gini\", \"entropy\"), \n",
        "    \"splitter\":(\"best\", \"random\"), \n",
        "    \"max_depth\":(list(range(1, 20))), \n",
        "    \"min_samples_split\":[2, 3, 4], \n",
        "    \"min_samples_leaf\":list(range(1, 20)), \n",
        "    \"class_weight\":(None,\"balanced\")\n",
        "    #\"max_features\":(list(range(1,len(df.columns)))),\n",
        "}\n",
        "\n",
        "\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "grid_search_cv = GridSearchCV(model, params, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=3)\n",
        "\n",
        "grid_search_cv.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYbQC6Y0FMpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " grid_search_cv.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2LvZ6K6FMpm",
        "colab_type": "text"
      },
      "source": [
        "The parameters for the best fit model suggested by Grid search is criterion = 'entropy', max_depth = 6, min_samples_leaf = 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh2LOSlQFMpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tree = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
        "                       max_depth=6, max_features=None, max_leaf_nodes=None,\n",
        "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                       min_samples_leaf=10, min_samples_split=2,\n",
        "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
        "                       random_state=42, splitter='best')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAB0RbgvFMpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tree.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "EQDoOo9dFMpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_score(tree, X_train, y_train, X_test, y_test, train=True)\n",
        "print_score(tree, X_train, y_train, X_test, y_test, train=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asB1BQCpFMpx",
        "colab_type": "text"
      },
      "source": [
        "The model obtained from the Grid search cross validation has a better F1 score than our model with optimal parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX9MdnIgFMpy",
        "colab_type": "text"
      },
      "source": [
        "### Visualization of a tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fdgCCOiFMpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = list(df.columns)\n",
        "classes=['Yes','No']\n",
        "features.remove(\"Attrition\")\n",
        "dot_data = export_graphviz(tree, out_file=None, \n",
        "                     feature_names=features,   \n",
        "                     class_names=classes,      \n",
        "                     filled=True, rounded=True,  \n",
        "                     special_characters=True)  \n",
        "graph = graphviz.Source(dot_data)  \n",
        "#graph.view()         #Uncomment text to view it as pdf\n",
        "graph\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEJTNbZHFMp1",
        "colab_type": "text"
      },
      "source": [
        "## 6. Feature Selection for Decision trees\n",
        "\n",
        "The task is to plot the feature importance of all the features in the dataset and find the best deatures which is useful in building the decision trees.\n",
        "\n",
        "Hint : Use sklearn.tree.feature_importances_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buOYZmZyFMp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(len(tree.feature_importances_))\n",
        "#print(len(df.columns))\n",
        "##################write your code here######################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE1tpq5aFMp5",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sciSE7vuFMp6",
        "colab_type": "text"
      },
      "source": [
        "## Additional Exercises\n",
        "\n",
        "Upsample or downsample the dataset and find the best fit model using GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thDkrTgbFMp7",
        "colab_type": "text"
      },
      "source": [
        "# 7. Summary\n",
        "In this notebook we learned the following lessons:\n",
        "- Decsion tree algorithms and the parameters of the algorithm.\n",
        "- How to tune hyperparameters for  Decision trees.\n",
        "- Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant. \n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej29Af7cFecv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}